{"cells":[{"cell_type":"markdown","source":["#Лабораторная работа №8"],"metadata":{"id":"HT3ffrbyhKRI"}},{"cell_type":"markdown","source":["#Задание"],"metadata":{"id":"MuM4Mmneg4qH"}},{"cell_type":"markdown","source":["Необходимо выполнить машинный перевод тектового корпуса для формирования набора данных. Обучить на сформированном наборе данных модель трасформер.\n","\n","Выполнить предсказание с помощью обученной модели. Проанализировать метрику BLEU"],"metadata":{"id":"2cGXG1SyH0MW"}},{"cell_type":"markdown","source":["Отчет должен содержать: титульный лист, задание с вариантом корпуса текста, скриншоты и краткие пояснения по каждому этапу лабораторной работы, результат BLEU.\n"],"metadata":{"id":"8oyW_66-H0MX"}},{"cell_type":"markdown","source":["#Контрольные вопросы\n","1. Машинный перевод\n","2. Архитектура трансформер\n","3. Механизм внимания\n","4. Архитектура seq2seq\n","5. Метрика BLEU"],"metadata":{"id":"OfptVMQOH0MX"}},{"cell_type":"markdown","source":["# Часть 1. Формирование корпуса для перевода с помощью предобученной модели"],"metadata":{"id":"wC8yc-79eWfz"}},{"cell_type":"markdown","source":["## Установка зависимостей"],"metadata":{"id":"IdhYW2SZf5US"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOsWldfBeQ-B","outputId":"9472fc40-0ed4-46fd-b757-20bc25620f6e","executionInfo":{"status":"ok","timestamp":1654037172630,"user_tz":-480,"elapsed":21388,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 4.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 38.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 56.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.4 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}]},{"cell_type":"markdown","source":["## Импорт библиотек"],"metadata":{"id":"nNPZHTcdf9MP"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"NFdgWWK3d8eH","executionInfo":{"status":"ok","timestamp":1654037195634,"user_tz":-480,"elapsed":6422,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchtext\n","from torchtext.datasets import Multi30k\n","\n","from transformers import MarianTokenizer, MarianMTModel\n","from typing import List\n","\n","from tqdm.auto import tqdm"]},{"cell_type":"markdown","source":["## Вывод информации о видеокарте"],"metadata":{"id":"gMrKX5tPg6Vw"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owBT-GWXg9vu","outputId":"dc70c243-7bf1-4035-d163-49f603e04b69","executionInfo":{"status":"ok","timestamp":1654037204932,"user_tz":-480,"elapsed":390,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}]},{"cell_type":"markdown","source":["## Загрузка и распаковка файлов датасета Multi30k"],"metadata":{"id":"G1ULRQQxf_kr"}},{"cell_type":"code","source":["!wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","!tar -xvzf validation.tar.gz\n","!wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz\n","!tar -xvzf training.tar.gz\n","!wget http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/mmt_task1_test2016.tar.gz\n","!tar -xvzf mmt_task1_test2016.tar.gz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QW59D8ULeDZY","outputId":"3497008c-af03-40e0-c415-b2cb061456c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-31 22:46:47--  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Resolving www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)... 143.167.8.76\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 22:48:59--  (try: 2)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 22:51:11--  (try: 3)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 22:53:23--  (try: 4)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 22:55:40--  (try: 5)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 22:57:56--  (try: 6)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 23:00:12--  (try: 7)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 23:02:28--  (try: 8)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 23:04:48--  (try: 9)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 23:07:09--  (try:10)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 23:09:29--  (try:11)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 23:11:48--  (try:12)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 23:14:07--  (try:13)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 23:16:27--  (try:14)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 23:18:46--  (try:15)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... failed: Connection timed out.\n","Retrying.\n","\n","--2022-05-31 23:21:05--  (try:16)  http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz\n","Connecting to www.quest.dcs.shef.ac.uk (www.quest.dcs.shef.ac.uk)|143.167.8.76|:80... "]}]},{"cell_type":"markdown","source":["## Машинный перевод\n","Как видно, в датасете только английский и немецкие языки\n","Для расширения числа языков, используем уже обученную модель и переведём английские предложения на целевой язык (по варианту)\n","\n","Список моделей можно изучить [тут](https://huggingface.co/Helsinki-NLP)\n"],"metadata":{"id":"CZiemtFCgHQW"}},{"cell_type":"code","source":["src = \"en\"  # исходный язык\n","trg = \"ru\"  # целевой язык\n","\n","model_name = f\"Helsinki-NLP/opus-mt-{src}-{trg}\"\n","\n","model = MarianMTModel.from_pretrained(model_name)\n","tokenizer = MarianTokenizer.from_pretrained(model_name)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","model.eval()\n","print(f'Модель переводчик с {src} на {trg} загружена!')"],"metadata":{"id":"rWkMS4FBeIv5","executionInfo":{"status":"aborted","timestamp":1654037176687,"user_tz":-480,"elapsed":68,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Пакетный перевод для каждой части выборки"],"metadata":{"id":"ZhIrcs08gcDj"}},{"cell_type":"code","source":["def translate_batch(text_batch):\n","    with torch.no_grad():\n","        # токенизируем список с предложениями\n","        batch = tokenizer(text_batch, return_tensors=\"pt\", padding=True)\n","        # помещаем батч на GPU\n","        batch = {k:v.to(device) for k,v in batch.items()}\n","        # переводим\n","        gen = model.generate(**batch)\n","        # декодируем из токенов полученные предложения\n","        return tokenizer.batch_decode(gen, skip_special_tokens=True)\n","\n","batch_size = 128\n","\n","for part in ['train', 'val', 'test2016']:\n","    text_batch = []\n","    output_corpus = []\n","    with open('%s.%s'%(part, src), 'r') as f:\n","        for line in tqdm(f):\n","            sample_text = line.strip()\n","            if len(sample_text) == 0: continue\n","            text_batch.append(sample_text)\n","            if len(text_batch) > batch_size - 1 :\n","                output_corpus.extend(translate_batch(text_batch))\n","                text_batch = []\n","    if len(text_batch):\n","        output_corpus.extend(translate_batch(text_batch))\n","    with open('%s.%s'%(part, trg), 'w') as f:\n","        f.write('\\n'.join(output_corpus))"],"metadata":{"id":"IWg5xEU9euAL","executionInfo":{"status":"aborted","timestamp":1654037176687,"user_tz":-480,"elapsed":68,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Упаковка расширенного корпуса по архивам"],"metadata":{"id":"jnIuz9M9gjXk"}},{"cell_type":"code","source":["!tar -czvf training.tar.gz train.*\n","!tar -czvf validation.tar.gz val.*\n","!tar -czvf mmt_task1_test2016.tar.gz test2016.*"],"metadata":{"id":"YpXHO_J6e5ct","executionInfo":{"status":"aborted","timestamp":1654037176688,"user_tz":-480,"elapsed":67,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Скачайте полученные файлы к себе на устройство и переходите к части 2."],"metadata":{"id":"QmKFvTPKgqkX"}},{"cell_type":"markdown","source":["#Часть 2. Transformer"],"metadata":{"id":"jicDNaUke6d1"}},{"cell_type":"markdown","metadata":{"id":"5famM3vcILji"},"source":["# Обучение ахитектуры трансформер\n","# Attention is All You Need\n","\n","В основу лабораторной положен ноутбук из репозитория https://github.com/bentrevett/pytorch-seq2seq\n","\n","В этом ноутбуке будет реализовывана (слегка измененная версия) модели Transformer из статьи [Attention is All You Need](https://arxiv.org/abs/1706.03762). Все изображения в этом блокноте будут взяты из статьи. Для получения дополнительной информации о Transformer, [читайте](https://www.mihaileric.com/posts/transformers-attention-in-disguise/) [эти](https://jalammar.github.io/illustrated-transformer/) [три](http://nlp.seas.harvard.edu/2018/04/03/attention.html) статьи.\n","\n","![](https://user-images.githubusercontent.com/47502256/155338911-46d70a40-d7d6-4742-9f97-15b49c4307a5.png)\n","\n","## Вступление\n","\n","Подобно сверточной модели последовательности в последовательность, модель трансформер не использует рекуррентных слоёв. Она также не использует никаких сверточных слоев. Вместо этого модель полностью состоит из полносвязных слоев, механизмов внимания и нормализации.\n","\n","По состоянию на январь 2022 года архитектура Transformer является доминирующей архитектурой в NLP и используются для достижения самых современных результатов для многих задач, и, видимо, так оно и будет в ближайшем будущем.\n","\n","Наиболее известный вариант трансформера это [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) и предварительно обученные версии BERT обычно используются для замены слоев эмбеддингов - если не чаще всего - в моделях NLP. \n","\n","Общей библиотекой, используемой при работе с предварительно обученными трансформерами является [Transformers](https://huggingface.co/transformers /) библиотека, смотрите [здесь](https://huggingface.co/transformers/pretrained_models.html ) для получения списка всех доступных предварительно обученных моделей.\n","\n","Различия между реализацией в этой лабораторной и в оригинальной статье заключаются в следующем:\n","- используется обучаемая позиционная кодировка вместо ручной статической\n","- используется стандартный оптимизатор Adam с неизменным шагом обучения вместо оптимизатора с прогревом (warmup) и охлаждением (cool-down) шага обучения.\n","- не используется сглаживание меток\n","\n","Все эти изменения вносятся, поскольку полностью соответствуют настройке BERT и поскольку большинство вариантов моделей трансформеров используют аналогичную настройку."]},{"cell_type":"markdown","metadata":{"id":"kK3KBRt0ILj1"},"source":["## Подготовка данных\n","\n","Импортируем все необходимые модули и установим начальные значения генератора случайных чисел seed для воспроизводимости.\n","Загрузите в Colab ранее расширенный набор данных Multi30k."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H03jsKPtILj2","executionInfo":{"status":"aborted","timestamp":1654037176689,"user_tz":-480,"elapsed":67,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchtext\n","from torchtext.legacy.datasets import Multi30k\n","from torchtext.legacy.data import Field, BucketIterator\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import spacy\n","from razdel import tokenize\n","import numpy as np\n","\n","import random\n","import math\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3F2laFiILj5","executionInfo":{"status":"aborted","timestamp":1654037176691,"user_tz":-480,"elapsed":68,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","source":["Далее загрузим модели spaCy и определим токенизаторы для исходного и целевого языков.\n","\n","Список поддерживаемых языков:\n","\n","https://spacy.io/models\n","\n","Библиотека раздел предназначена для токенизации русской речи.\n","\n","После установки перезагрузить среду: \n","\n","__Runtime -> Restart Runtime__\n","или __Среда выполнения -> перезапустить среду выполнения__."],"metadata":{"id":"gB1EMm7pC12Z"}},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm\n","!python -m spacy download de_core_news_sm\n","!pip install razdel"],"metadata":{"id":"BD_X5LJfC6rU","executionInfo":{"status":"aborted","timestamp":1654037176692,"user_tz":-480,"elapsed":65,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GNlk461AILj7"},"source":["Затем создадим токенизаторы"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkvwspHzILj8","executionInfo":{"status":"aborted","timestamp":1654037176695,"user_tz":-480,"elapsed":66,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["spacy_de = spacy.load('de_core_news_sm')\n","spacy_en = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZcNeXYuILj-","executionInfo":{"status":"aborted","timestamp":1654037176697,"user_tz":-480,"elapsed":66,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["from razdel import tokenize\n","import re\n","\n","def tokenize_de(text):\n","    \"\"\"\n","    Tokenizes German text from a string into a list of strings\n","    \"\"\"\n","    return [tok.text for tok in spacy_de.tokenizer(text)]\n","\n","def tokenize_en(text):\n","    \"\"\"\n","    Tokenizes English text from a string into a list of strings\n","    \"\"\"\n","    return [tok.text for tok in spacy_en.tokenizer(text)]\n","\n","def tokenize_ru(text):\n","    return [tok.text for tok in tokenize(text)]\n","\n","def tokenize_regex(text):\n","    return re.findall(\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\",text)\n","\n","tokenize_regex('Простое предложение для токенизации')"]},{"cell_type":"markdown","metadata":{"id":"ZbZrzzrAILkA"},"source":["Field такие же, как и в предыдущей части лабораторной. Модель ожидает данные, где batch идёт первым, поэтому используется флаг `batch_first = True`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RBIONxeILkB","executionInfo":{"status":"aborted","timestamp":1654037176701,"user_tz":-480,"elapsed":69,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["SRC = Field(tokenize = tokenize_ru, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True, \n","            batch_first = True)\n","\n","TRG = Field(tokenize = tokenize_en, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True, \n","            batch_first = True)"]},{"cell_type":"markdown","metadata":{"id":"s-3_tv5fILkD"},"source":["Теперь выполним загрузку набора данных, полученных в первой части лабораторной работы"]},{"cell_type":"code","source":["!gdown --id 1YWbueklT5VgNhjuxoapZFnkRSwPxH_r0\n","!gdown --id 1X1aXa_VMCkyAox5OABqN132AZx7Hw4uq\n","!gdown --id 1nF8ff7QniRCkP-ybDKXcAMKk8YQaHA7Q\n","!tar -xvzf validation.tar.gz\n","!tar -xvzf training.tar.gz\n","!tar -xvzf mmt_task1_test2016.tar.gz"],"metadata":{"id":"zz2rwIm9Ottf","executionInfo":{"status":"aborted","timestamp":1654037176703,"user_tz":-480,"elapsed":70,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, valid_data, test_data = Multi30k.splits(exts=('.ru', '.en'), \n","                                                    fields=(SRC, TRG),\n","                                                    path='')"],"metadata":{"id":"BTN4NTuXj8UZ","executionInfo":{"status":"aborted","timestamp":1654037176705,"user_tz":-480,"elapsed":71,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iv7NPE_eILkF","executionInfo":{"status":"aborted","timestamp":1654037176706,"user_tz":-480,"elapsed":71,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["SRC.build_vocab(train_data, min_freq = 2)\n","TRG.build_vocab(train_data, min_freq = 2)"]},{"cell_type":"markdown","metadata":{"id":"GATligcvILkG"},"source":["Наконец, определим устройство и итератор данных."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIjDyr0xILkH","executionInfo":{"status":"aborted","timestamp":1654037176708,"user_tz":-480,"elapsed":72,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Gnk9r9tILkH","executionInfo":{"status":"aborted","timestamp":1654037176709,"user_tz":-480,"elapsed":72,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["BATCH_SIZE = 128\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","     batch_size = BATCH_SIZE,\n","     device = device)"]},{"cell_type":"markdown","metadata":{"id":"jMqN45REILkI"},"source":["## Создание модели\n","\n","Пришло время создать модель. Как и в предыдущей части лабораторной, он состоит из *кодировщика * и *декодировщика *, при этом энкодер *кодирует* входящее предложение/источник (на русском языке) в *контекстный вектор*, а затем декодер *расшифровывает* этот контекстный вектор для вывода выходного/целевого предложения (на английском языке).\n","\n","### Энкодер\n","\n","Подобно модели ConvSeq2Seq, кодировщик Transformer не пытается сжать все исходное предложение, $X = (x_1, ... ,x_n)$, в один контекстный вектор, $z$. Вместо этого он создает последовательность контекстных векторов, $Z = (z_1, ... , z_n)$. Итак, если бы входная последовательность состояла из 5 токенов, то было бы $Z = (z_1, z_2, z_3, z_4, z_5)$. Почему это называется последовательностью контекстных векторов, а не последовательностью скрытых состояний? Скрытое состояние в момент времени $t$ в RNN видит только токены $x_t$ и все токены до него. Однако каждый контекстный вектор здесь видел все токены во всех позициях входной последовательности.\n","\n","![](https://user-images.githubusercontent.com/47502256/155342415-4d6f847f-22c3-4cd8-8433-00644ce6e8cd.png)\n","\n","Во-первых, токены передаются через стандартный слой эмбеддинг. Далее, поскольку модель не имеет рекуррентных слоёв, она понятия не имеет о порядке расположения токенов в последовательности. Эта проблема решается с помощью второго слоя эмбеддинга, называемого *позиционным слоем эмбеддинга*. Это обычный слой эмбеддинг, где входом является не сам токен, а позиция токена в последовательности, начиная с первого токена, токена \"<sos>\" (начало последовательности), в позиции 0. Эмбеддинг позиции имеет размер \"словарного запаса\" 100, что означает, что наша модель может принимать предложения длиной до 100 токенов. Длину можно увеличить, если требуется обрабатывать более длинные предложения.\n","\n","Оригинальная реализация Transformer от Attention - is all you need не изучает позиционные эмбеддинги. Вместо этого он использует фиксированные заданные вручную. Современные архитектуры Transformer, такие как BERT, вместо этого используют обучаемые эмбеддинги, поэтому в данной работе используются они. Изучите [этот](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding ) ресурс, чтобы узнать больше о позиционных эмбеддингах, используемых в исходной модели трансформатора.\n","\n","Затем эмбеддинги токенов и позиций поэлементно суммируются, чтобы получить вектор, который содержит информацию о токене, а также его положении в последовательности. Однако, прежде чем они будут суммированы, эмбеддинги токенов умножаются на коэффициент масштабирования равный $\\sqrt{d_{model}}$, где $d_{model}$ - размер скрытого измерения `hid_dim`. Говорят, что это уменьшает дисперсию в эмбеддингах и модель легче обучать с этим коэффициентом масштабирования. Наконец, к объединенным эмбеддингам применяется слой Dropout.\n","\n","Объединенные вложения затем передаются через $ N $ *слои кодировщики*, чтобы получить $Z$, который является выходом и подаётся на декодер.\n","\n","Исходная маска `src_mask` (иначе называется, маска внимания, attention mask) имеет ту же размерность, что и исходное предложение, но имеет значение 1, если токен в исходном предложении не является токеном `<pad>`, и 0, когда это токен `<pad>`. Маска используется в слоях кодировщика для маскировки механизмов внимания с несколькими головами (multi-head attention), которые используются для вычисления и применения внимания к исходному предложению, поэтому модель не обращает внимания на токены \"<pad>\", которые не содержат полезной информации."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPvEFyO3ILkL","executionInfo":{"status":"aborted","timestamp":1654037176710,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, \n","                 input_dim, \n","                 hid_dim, \n","                 n_layers, \n","                 n_heads, \n","                 pf_dim,\n","                 dropout, \n","                 device,\n","                 max_length = 100):\n","        super().__init__()\n","\n","        self.device = device\n","        \n","        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        \n","        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n","                                                  n_heads, \n","                                                  pf_dim,\n","                                                  dropout, \n","                                                  device) \n","                                     for _ in range(n_layers)])\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","        \n","    def forward(self, src, src_mask):\n","        \n","        #src = [batch size, src len]\n","        #src_mask = [batch size, 1, 1, src len]\n","        \n","        batch_size = src.shape[0]\n","        src_len = src.shape[1]\n","        \n","        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","        \n","        #pos = [batch size, src len]\n","        \n","        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        for layer in self.layers:\n","            src = layer(src, src_mask)\n","            \n","        #src = [batch size, src len, hid dim]\n","            \n","        return src"]},{"cell_type":"markdown","metadata":{"id":"fGF8lAfmILkO"},"source":["### Слой кодировщик\n","\n","Слои кодировщика - это то место, где содержится все \"мясо\" энкодера. Сначала передается исходное предложение и его маска в *слой внимания с несколькими головами*, затем применяется dropout, соединение с пробросом (residual connection) и применение слоя [Нормализация слоя](https://arxiv.org/abs/1607.06450 ). Затем пропускаем его через *полносвязный слой по позиции*, затем снова droupout, соединение с пробросом и нормализация слоя для получения выходных данных кодировщика. Они затем передаются в следующий слой. Параметры не распределяются между слоями (как, например, в архитектуре ALBERT). Т.е. каждый слой учит свои параметры.\n","\n","Слой внимания __MultiHeadAttentionLayer__ используется слоем энкодера для внимания к исходному предложению, т.е. он вычисляет и применяет внимание к самому себе, а не другой последовательности, поэтому это называется механизмом \"самовнимания\".\n","\n","[Данная](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/) статья более подробно рассказывает о нормализации слоя. Суть в том, что она нормализует значения объектов, то есть по скрытому измерению, поэтому каждый объект имеет среднее значение 0 и стандартное отклонение 1. Это позволяет легче обучать нейронные сети с большим количеством слоев, такие как Transformer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RoKA7RVxILkP","executionInfo":{"status":"aborted","timestamp":1654037176710,"user_tz":-480,"elapsed":72,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim,  \n","                 dropout, \n","                 device):\n","        super().__init__()\n","        \n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src, src_mask):\n","        \n","        #src = [batch size, src len, hid dim]\n","        #src_mask = [batch size, 1, 1, src len] \n","                \n","        #self attention\n","        _src, _ = self.self_attention(src, src, src, src_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        src = self.self_attn_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        #positionwise feedforward\n","        _src = self.positionwise_feedforward(src)\n","        \n","        #dropout, residual and layer norm\n","        src = self.ff_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        return src"]},{"cell_type":"markdown","metadata":{"id":"6zZJrSRWILkR"},"source":["### Слой Внимания с множеством голов\n","\n","Одной из ключевых, новых концепций, представленных в работе по трансформеру является *многоголовый слой внимания*.\n","\n","![](https://user-images.githubusercontent.com/47502256/155346480-870357aa-24c9-4054-80f8-3952573655c5.png)\n","\n","Внимание может быть представлено в виде *запросов*, *ключей* и *значений* - где запрос используется с ключом для получения вектора внимания (обычно это результат операции *softmax* со значениями в диапазоне от 0 до 1 с суммой равной 1), который затем используется для получения взвешенной суммы значений.\n","\n","Трансформер использует *масштабированное скалярное произведение внимания*, где запрос и ключ объединяются путем взятия скалярного произведения между ними, затем применения операции softmax и масштабирования на $d_k$ перед окончательным умножением на значение. $d_k$ - это *измерение головы*, `head_dim`, которое будет кратко объяснено далее.\n","\n","$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ \n","\n","Формула похожа на классическое *скалярное произведение внимания*, но с коэффициентом масштабирование на $d_k$. В оригинальной статье это предлагалось делать с целью предотвращения больших значений выражения под скобками. Большие значения могли привести к тому, что сигнал обратного распространения при обучении будет слишком слабым.\n","\n","Однако внимание к масштабируемому скалярному произведению не заканчивается простым применением к запросам, ключам и значениям. Вместо того, чтобы применить один механизм внимания, запросы, ключи и значения разделяют `hid_dim` на $h$ *голов*, и масштабированное скалярное произведение внимания вычисляется по всем головам параллельно. Это означает, что вместо того, чтобы обращать внимание на одну область, внимание обращается внимание на $ h $. Затем они объединяются в первоначальную размерность `hid_dim`. Таким образом, каждая голова `hid_dim` потенциально обращает внимание на разные области $ h $.\n","\n","$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n","\n","$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n","\n","где $W^O$ это полносвязный слой, который применяется в конце слоя многоголового внимания, `fc`. $W^Q, W^K, W^V$ это полносвязные слои `fc_q`, `fc_k` и `fc_v`.\n","\n","Проходя по блоку внимания, сначала вычисляется $QW ^ Q$, $KW ^ K$ и $ VW ^ V$ с линейными слоями `fc_q`, `fc_k` и `fc_v`, чтобы получить `Q`, `K` и `V`. Затем запрос, ключ и значение `hid_dim` делится на `n_heads` с помощью `.view` и правильно транспонируются при надобности, чтобы их можно было перемножить друг с другом. Затем вычисляется мера \"энергии\" (ненормированное внимание), скалярное произведение \"Q\" и \"K\" вместе помноженный на квадратный корень из \"head_dim\", который вычисляется как \"hid_dim // n_heads\". К этой величине применяется маска, чтобы не обращать внимания на элементы последовательности, которые не интересны (токены '<pad>'), затем применяется softmax и dropout. Затем происходит обращение внимания на заголовки значений `V`, прежде чем объединять `n_heads` вместе. Наконец, результат умножается на $W ^ O$, представленное `fc_o`.\n","\n","Обратите внимание, что в данной реализации длины ключей и значений всегда одинаковы, поэтому при матричном умножении выходных данных softmax, `attention`, у `V` всегда будут та же размерность измерений для перемножения матриц. Это перемножение выполняется с помощью `torch.matmul`, который, когда оба тензора > 2-мерные, выполняет пакетное матричное перемножение по последним двум измерениям каждого тензора. Это будет **[длина запроса, длина ключа] x [длина значения, длина заголовка]** пакетное перемножение матриц на размер пакета и каждую голову. В результате **[размер пакета, число заголовков, длина запроса, размерность заголовка]**.\n","\n","Кое-какая вещь может показаться странной в начале - dropout применятся напрямую к механизму внимания. Это означает, что вектор внимания вероятнее всего не суммируется к 1 и что может быть максимальное внимание к токену, но dropout его занулит. Это никогда не оговаривлось или упоминалось в работе, однако используется в первоначальной [официальной реализации](https://github.com/tensorflow/tensor2tensor/) и каждой последующей реализации архитектуры трансформера, [включая BERT](https://github.com/google-research/bert/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MNYWgP5BILkS","executionInfo":{"status":"aborted","timestamp":1654037176711,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout, device):\n","        super().__init__()\n","        \n","        assert hid_dim % n_heads == 0\n","        \n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","        self.head_dim = hid_dim // n_heads\n","        \n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","        \n","    def forward(self, query, key, value, mask = None):\n","        \n","        batch_size = query.shape[0]\n","        \n","        #query = [batch size, query len, hid dim]\n","        #key = [batch size, key len, hid dim]\n","        #value = [batch size, value len, hid dim]\n","                \n","        Q = self.fc_q(query)\n","        K = self.fc_k(key)\n","        V = self.fc_v(value)\n","        \n","        #Q = [batch size, query len, hid dim]\n","        #K = [batch size, key len, hid dim]\n","        #V = [batch size, value len, hid dim]\n","                \n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        \n","        #Q = [batch size, n heads, query len, head dim]\n","        #K = [batch size, n heads, key len, head dim]\n","        #V = [batch size, n heads, value len, head dim]\n","                \n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","        \n","        #energy = [batch size, n heads, query len, key len]\n","        \n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, -1e10)\n","        \n","        attention = torch.softmax(energy, dim = -1)\n","                \n","        #attention = [batch size, n heads, query len, key len]\n","                \n","        x = torch.matmul(self.dropout(attention), V)\n","        \n","        #x = [batch size, n heads, query len, head dim]\n","        \n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        \n","        #x = [batch size, query len, n heads, head dim]\n","        \n","        x = x.view(batch_size, -1, self.hid_dim)\n","        \n","        #x = [batch size, query len, hid dim]\n","        \n","        x = self.fc_o(x)\n","        \n","        #x = [batch size, query len, hid dim]\n","        \n","        return x, attention"]},{"cell_type":"markdown","metadata":{"id":"FjcYRD4RILkT"},"source":["### Полносвязный слой Положения\n","\n","Другим основным блоком внутри уровня кодировщика является *Полносвязный слой Положения* Это относительно простой по сравнению с многоголовым слоем внимания. Входные данные преобразуются из `hid_dim` в `pf_dim`, где `pf_dim` обычно намного больше, чем `hid_dim`. Оригинальный трансформер использовал `hid_dim`, равный 512, и `pf_dim`, равный 2048. Функция активации ReLU и dropout применяются до того, как она будет преобразована обратно в представление `hid_dim`.\n","\n","Почему это используется? К сожалению, это не объясняется в работе.\n","\n","BERT использует активационную функцию [GELU](https://arxiv.org/abs/1606.08415), которую можно заменить с `torch.relu` на `F.gelu`. GELU это разработка исследователей Google, поэтому при любой удобной возможности они её рекламируют в архитектурах."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwb1tAX_ILkT","executionInfo":{"status":"aborted","timestamp":1654037176711,"user_tz":-480,"elapsed":72,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","        \n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        x = self.dropout(torch.relu(self.fc_1(x)))\n","        \n","        #x = [batch size, seq len, pf dim]\n","        \n","        x = self.fc_2(x)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        return x"]},{"cell_type":"markdown","metadata":{"id":"0KsLwf3KILkU"},"source":["### Декодер\n","\n","Цель декодера состоит в том, чтобы взять закодированное представление исходного предложения, $Z$, и преобразовать его в предсказанные токены в целевом предложении, $\\hat {Y}$. Затем сравниваются $\\hat {Y}$ с фактическими токенами в целевом предложении, $Y$, чтобы рассчитать функцию потери. При помощи расчитанного обратного сигнала оптимизатор для обновляет веса модели для улучшения наши прогнозов.\n","\n","![](https://user-images.githubusercontent.com/47502256/155352836-d04e0629-5844-46cc-8bb5-d9a6ab049e1e.png)\n","\n","Декодер похож на кодировщик, однако теперь с двумя блоками многоголового внимания. *Максированный многоголовый слой внимания* применяется к целевой последовательности, а другой использует представление декодера в качестве запроса и представление кодировщика в качестве ключа и значения.\n","\n","Декодер использует эмбеддинги положения, которые суммируются с целевыми отмасштабированными токенами, с последующим применением droupout. И вновь, представления положения имеют размер \"словаря\" 100, что означается, что могут использоваться последовательности длительностью до 100 токенов. Что можно увеличить при необходимости.\n","\n","Объединённые эмбеддинги затем подаются через $N$ слоёв декодера, совместо с маской энкодера, `enc_src`, маской источника и целевой маской. Заметьте, что число слоёв в энкодере не обязательно должно равняться числу слоёв в декодере, хоть тут и используется то же обозначение $N$.\n","\n","Представление декодера после $N^{th}$-го слоя затем подаётся на полносвязный слой, `fc_out`. В PyTorch, операция softmax встроена в функцию потерь, поэтому нет надобности применять её к выходному слою.\n","\n","Целевая маска используется с той же целью, что и исходная и маска энкодера. Для нивелирования влияния `<pad>` токенов. При описании архитектуры `Seq2Seq` это будет затронуто более подробно. Однако смысл в том, что выполняется похожая операция на заполнение декодера полностью свёрточной sequence-to-sequence модели. Поскольку все целевые токены обрабатываются одновременно, необходим метод, который не позволит декодеру \"жульничать\" путём \"подсматривания\" на следующий токен и предсказание его же. \n","\n","Слой декодер также возвращает нормализованные значения внимания. Что даёт возможность визуализировать, на что смотрит модель при формировании ответа."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiqjGtjrILkV","executionInfo":{"status":"aborted","timestamp":1654037176712,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, \n","                 output_dim, \n","                 hid_dim, \n","                 n_layers, \n","                 n_heads, \n","                 pf_dim, \n","                 dropout, \n","                 device,\n","                 max_length = 100):\n","        super().__init__()\n","        \n","        self.device = device\n","        \n","        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        \n","        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n","                                                  n_heads, \n","                                                  pf_dim, \n","                                                  dropout, \n","                                                  device)\n","                                     for _ in range(n_layers)])\n","        \n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","        \n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        \n","        #trg = [batch size, trg len]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        #src_mask = [batch size, 1, 1, src len]\n","                \n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        \n","        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","                            \n","        #pos = [batch size, trg len]\n","            \n","        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n","                \n","        #trg = [batch size, trg len, hid dim]\n","        \n","        for layer in self.layers:\n","            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        output = self.fc_out(trg)\n","        \n","        #output = [batch size, trg len, output dim]\n","            \n","        return output, attention"]},{"cell_type":"markdown","metadata":{"id":"7xuEfA6qILkV"},"source":["### Слой декодера\n","\n","Как упомянуто ранее, слой декодера похож на слой энкодераза исключением того факта, что он имеет два слоя многоголового внимания, `self_attention` и `encoder_attention`.\n","\n","Первый выполняет механизм самовнимание, как в энкодере, используя представление декодера как запрос, ключ и значение. Плюс dropout, соединение с пробросом и нормализация слоя. Данный слой `self_attention` использует маску целевой последовательности `trg_mask`, чтобы избежать \"жульничества\" путём акцентирования внимания на токенах, которые идут следом за тем, которые в данный момент обрабатывается.\n","\n","Второй - это то, как на самом деле передаётся закодированное исходное предложение \"enc_src\" в декодер. В этом многоголовом слое внимания запросы являются представлениями декодера, а ключи и значения являются представлениями энкодера. Здесь исходная маска `src_mask` используется также для предотвращения жульничества, чтобы слой внимания с несколькими головами не обращал внимания на токены `<pad>` в исходном предложении. Затем опять dropout, соединение с пробросом и слой нормализация.\n","\n","Наконец, результат подаётся на вход в полносвязный слой положения и опять  droupout, соединение с пробросом и нормализация слоя.\n","\n","Слой декодера не вводит никаких новых концепций, просто использует тот же набор слоев, что и энкодер, немного по-другому."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsxotwpjILkW","executionInfo":{"status":"aborted","timestamp":1654037176712,"user_tz":-480,"elapsed":72,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim, \n","                 dropout, \n","                 device):\n","        super().__init__()\n","        \n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        #src_mask = [batch size, 1, 1, src len]\n","        \n","        #self attention\n","        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n","            \n","        #trg = [batch size, trg len, hid dim]\n","            \n","        #encoder attention\n","        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n","                    \n","        #trg = [batch size, trg len, hid dim]\n","        \n","        #positionwise feedforward\n","        _trg = self.positionwise_feedforward(trg)\n","        \n","        #dropout, residual and layer norm\n","        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        return trg, attention"]},{"cell_type":"markdown","metadata":{"id":"XTh1j4rmILkX"},"source":["### Seq2Seq\n","\n","Наконец, рассмотрим модель Seq2Seq`, которая инкапсуляции энкодер и декодер, а также создаёт маски.\n","\n","Исходная маска создаётся проверкой исходной последовательности на равенство `<pad>` токенов. 1 где не `<pad>` токен и 0 где указан токен заполнения. Затем  выполняется дублирование, чтобы совпадали размерность и не было ошибок во время применения маски к `энергии`, которая имеет форму **_[batch size, n heads, seq len, seq len]_**.\n","\n","Целевая маска немного более сложная. Вначале, по аналогии создается маска, игнорирующая токены `<pad>`. Затем создается  \"урезанная\" маска, `trg_sub_mask` при помощи `torch.tril`. В результате получается диагональная матрица, где элементы выше диагонали будут нулями, а элементы ниже диагонали сохраняют своё первоначальное значение. В случае маски входной тензор будет заполнен единицами. Пример, как будет выглядеть урезанная маска `trg_sub_mask` (для последовательности из 5 токенов):\n","\n","$$\\begin{matrix}\n","1 & 0 & 0 & 0 & 0\\\\\n","1 & 1 & 0 & 0 & 0\\\\\n","1 & 1 & 1 & 0 & 0\\\\\n","1 & 1 & 1 & 1 & 0\\\\\n","1 & 1 & 1 & 1 & 1\\\\\n","\\end{matrix}$$\n","\n","Это показывает, что разрешается просматривать каждому целевому токену (строке) (столбцу). Первый целевой токен имеет маску **_[1, 0, 0, 0, 0]_** Что означает, что он может просматривать только первый целевой токен. Второй целевой токен имеет маску **_[1, 1, 0, 0, 0]_**, что означает, что он может просматривать как первый, так и второй целевые токены.\n","\n","Урезанная маска затем поэлементно совмещается с маской заполнения с помощью логической операции \"И\". Например, если последние два токена токены были `<pad>`, итоговая маска будет выглядить следующим образом:\n","\n","$$\\begin{matrix}\n","1 & 0 & 0 & 0 & 0\\\\\n","1 & 1 & 0 & 0 & 0\\\\\n","1 & 1 & 1 & 0 & 0\\\\\n","1 & 1 & 1 & 0 & 0\\\\\n","1 & 1 & 1 & 0 & 0\\\\\n","\\end{matrix}$$\n","\n","После того, как маски созданы, они используются в энкодере и декодере совместно с исходными и целевыми последовательностями для получения предсказанной целевой последовательности `output`, вместе с вниманием декодера к исходной последовательности."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8ejsTchILkY","executionInfo":{"status":"aborted","timestamp":1654037176713,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, \n","                 encoder, \n","                 decoder, \n","                 src_pad_idx, \n","                 trg_pad_idx, \n","                 device):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","        \n","    def make_src_mask(self, src):\n","        \n","        #src = [batch size, src len]\n","        \n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        return src_mask\n","    \n","    def make_trg_mask(self, trg):\n","        \n","        #trg = [batch size, trg len]\n","        \n","        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n","        \n","        #trg_pad_mask = [batch size, 1, 1, trg len]\n","        \n","        trg_len = trg.shape[1]\n","        \n","        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n","        \n","        #trg_sub_mask = [trg len, trg len]\n","            \n","        trg_mask = trg_pad_mask & trg_sub_mask\n","        \n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        \n","        return trg_mask\n","\n","    def forward(self, src, trg):\n","        \n","        #src = [batch size, src len]\n","        #trg = [batch size, trg len]\n","                \n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        \n","        #src_mask = [batch size, 1, 1, src len]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        \n","        enc_src = self.encoder(src, src_mask)\n","        \n","        #enc_src = [batch size, src len, hid dim]\n","                \n","        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n","        \n","        #output = [batch size, trg len, output dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        return output, attention"]},{"cell_type":"markdown","metadata":{"id":"e57oncEPILkY"},"source":["## Обучение модели Seq2Seq\n","\n","Теперь определим энкодер и декодер. Эта модель значительно меньше трансформеров, используемых сегодня в исследованиях, но способна быстро работать на одном графическом процессоре."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UILELu0oILka","executionInfo":{"status":"aborted","timestamp":1654037176713,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","HID_DIM = 256\n","ENC_LAYERS = 3\n","DEC_LAYERS = 3\n","ENC_HEADS = 8\n","DEC_HEADS = 8\n","ENC_PF_DIM = 512\n","DEC_PF_DIM = 512\n","ENC_DROPOUT = 0.15\n","DEC_DROPOUT = 0.15\n","\n","enc = Encoder(INPUT_DIM, \n","              HID_DIM, \n","              ENC_LAYERS, \n","              ENC_HEADS, \n","              ENC_PF_DIM, \n","              ENC_DROPOUT, \n","              device)\n","\n","dec = Decoder(OUTPUT_DIM, \n","              HID_DIM, \n","              DEC_LAYERS, \n","              DEC_HEADS, \n","              DEC_PF_DIM, \n","              DEC_DROPOUT, \n","              device)"]},{"cell_type":"markdown","metadata":{"id":"wlYMg1p1ILka"},"source":["Затем используйте их для определения всей нашей модели инкапсуляции последовательности в последовательность."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BN11owzeILka","executionInfo":{"status":"aborted","timestamp":1654037176714,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","\n","model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"]},{"cell_type":"markdown","metadata":{"id":"yEkqADW2ILkb"},"source":["Можно проверить количество параметров, заметив, что оно значительно меньше, чем 37M для сверточной модели Seq2Seq."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OV87_mTbILkc","executionInfo":{"status":"aborted","timestamp":1654037176714,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'Модель содержит {count_parameters(model):,} параметров для обучения')"]},{"cell_type":"markdown","metadata":{"id":"uHvZhs2XILkc"},"source":["В работе не упоминается, какая схема инициализации весов использовалась, однако Xavier распространена среди моделей трансформеров, поэтому используем ее и здесь."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L280NF93ILkd","executionInfo":{"status":"aborted","timestamp":1654037176715,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["def initialize_weights(m):\n","    if hasattr(m, 'weight') and m.weight.dim() > 1:\n","        nn.init.xavier_uniform_(m.weight.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzQS8PUkILkd","executionInfo":{"status":"aborted","timestamp":1654037176715,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["model.apply(initialize_weights);"]},{"cell_type":"markdown","metadata":{"id":"zMIYtdepILkd"},"source":["Оптимизатор, используемый в оригинальной статье Transformer, использует Adam со скоростью обучения, которая имеет период \"разогрева\", а затем период \"охлаждения\". BERT и другие модели используют Adam с фиксированной скоростью обучения. При желании изучите [эту](http://nlp.seas.harvard.edu/2018/04/03/attention.html#optimizer ) ссылку для получения более подробной информации о графике обучения оригинального трансформера.\n","\n","Обратите внимание, что скорость обучения должна быть ниже, чем по умолчанию, используемая Adam (1e-3), иначе обучение будет нестабильным."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zUr867kILke","executionInfo":{"status":"aborted","timestamp":1654037176716,"user_tz":-480,"elapsed":74,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["LEARNING_RATE = 5e-4#0.0005\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{"id":"aYfE2GzuILke"},"source":["Далее мы определяем нашу функцию потерь, следя за тем, чтобы игнорировать потери, рассчитанные по токенам `<pad>`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExigJQLuILke","executionInfo":{"status":"aborted","timestamp":1654037176717,"user_tz":-480,"elapsed":72,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"]},{"cell_type":"markdown","metadata":{"id":"qI585AFsILkf"},"source":["Затем определим цикл обучения. Он аналогичен прошлой части лабораторной работы.\n","\n","Поскольку требуется, чтобы модель предсказывала токен \"<eos>\", но не включала его в модель, то просто отсекаем токен \"<eos>\" от конца последовательности. Таким образом:\n","\n","$$\\begin{align*}\n","\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n","\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n","\\end{align*}$$\n","$x_i$ обозначает фактический элемент целевой последовательности. Затем подаём его в модель, чтобы получить предсказанную последовательность, которая должна предсказать токен \"<eos>\":\n","\n","$$\\begin{align*}\n","\\text{output} &= [y_1, y_2, y_3, eos]\n","\\end{align*}$$\n","\n","\n","$y_i$ обозначает предсказанный элемент целевой последовательности. Затем вычисляется функция потери, используя исходный тензор `trg` с токеном `<sos>`, отрезанным спереди, оставляя токен `<eos>`:\n","\n","$$\\begin{align*}\n","\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n","\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n","\\end{align*}$$\n","\n","Затем получаем значение потерь и обновляем параметры в соответствии со алгоритмом обратного распространения."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFpRrY7PILkg","executionInfo":{"status":"aborted","timestamp":1654037176718,"user_tz":-480,"elapsed":72,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        src = batch.src\n","        trg = batch.trg\n","        \n","        optimizer.zero_grad()\n","        \n","        output, _ = model(src, trg[:,:-1])\n","                \n","        #output = [batch size, trg len - 1, output dim]\n","        #trg = [batch size, trg len]\n","            \n","        output_dim = output.shape[-1]\n","            \n","        output = output.contiguous().view(-1, output_dim)\n","        trg = trg[:,1:].contiguous().view(-1)\n","                \n","        #output = [batch size * trg len - 1, output dim]\n","        #trg = [batch size * trg len - 1]\n","            \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{"id":"gBC2mddZILkh"},"source":["Цикл оценки такой же, как и цикл обучения, только без вычислений градиента и обновления параметров."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iw0QLYCvILkh","executionInfo":{"status":"aborted","timestamp":1654037176719,"user_tz":-480,"elapsed":72,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output, _ = model(src, trg[:,:-1])\n","            \n","            #output = [batch size, trg len - 1, output dim]\n","            #trg = [batch size, trg len]\n","            \n","            output_dim = output.shape[-1]\n","            \n","            output = output.contiguous().view(-1, output_dim)\n","            trg = trg[:,1:].contiguous().view(-1)\n","            \n","            #output = [batch size * trg len - 1, output dim]\n","            #trg = [batch size * trg len - 1]\n","            \n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{"id":"BS_nBQ1CILki"},"source":["Затем определим небольшую функцию, которую используем для информирования времени прохождения одной эпохи."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5d3BMNh3ILkj","executionInfo":{"status":"aborted","timestamp":1654037176720,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"markdown","metadata":{"id":"dToMFnbaILkk"},"source":["Наконец, запускаем обучение модели. Эта модель быстрее, чем сверточная модель Seq2Seq, а также обеспечивает меньшую perplexity на валидации!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ut61KJxHILkk","executionInfo":{"status":"aborted","timestamp":1654037176720,"user_tz":-480,"elapsed":72,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["N_EPOCHS = 15\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut6-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"]},{"cell_type":"markdown","metadata":{"id":"57jruQN_ILkk"},"source":["Загрузим \"лучшие\" параметры и добиваемся лучшей perplexity, чем предыдущая модель."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cp7aHSlbILkl","executionInfo":{"status":"aborted","timestamp":1654037176721,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["model.load_state_dict(torch.load('tut6-model.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"]},{"cell_type":"markdown","metadata":{"id":"dNa6ZMqzILkl"},"source":["## Предсказание\n","\n","Теперь можно выполнять переводы моделью с помощью функции translate_sentence, приведенной ниже.\n","\n","Предпринятые шаги заключаются в следующем:\n","\n","**Шаг 1.** токенизировать исходное предложение, если оно не было токенизировано (т.е. строка, а не массив токенов)\n","\n","**Шаг 2.** добавляем токены `<sos>` и `<eos>`.\n","\n","**Шаг 3.** пронумеровать исходное предложение\n","\n","**Шаг 4.** преобразовать его в тензор и добавьте измерение батча\n","\n","**Шаг 5.** создать маску исходного предложения\n","\n","**Шаг 6.** ввести исходное предложение и маску в кодировщик\n","\n","**Шаг 7.** создать список для хранения выходного предложения, инициализированного токеном `<sos>`.\n","\n","**Шаг 8.** пока максимальная длина не достигнута \n","\n","**Шаг 9.** преобразовать текущее прогнозирование выходного предложения в тензор с размерностью батча\n","\n","**Шаг 10.** создать маску целевого предложения\n","\n","**Шаг 11.** поместить текущий выход, выход энкодера и обе маски в декодер\n","\n","**Шаг 12.** получить прогноз следующего выходного токена от декодера вместе с вниманием\n","\n","**Шаг 13.** добавить прогноз к текущему прогнозу выходного предложения\n","\n","**Шаг 14.** завершение, если предсказание было токеном \"<eos>\"\n","\n","**Шаг 15.** преобразовать результирующее предложение из индексов в токены\n","\n","**Шаг 16.** вернуть результирующее предложение (с удаленным символом \"<sos>\") и внимание с последнего слоя"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLX9nI-4ILkm","executionInfo":{"status":"aborted","timestamp":1654037176722,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n","    \n","    model.eval()\n","        \n","    if isinstance(sentence, str):\n","        # ТУТ СВОЙ ВХОДНОЙ ТОКЕНИЗАТОР\n","        #nlp = spacy.load('de_core_news_sm')\n","        nlp = tokenize\n","        tokens = [token.text.lower() for token in nlp(sentence)]\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","        \n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n","    \n","    src_mask = model.make_src_mask(src_tensor)\n","    \n","    with torch.no_grad():\n","        enc_src = model.encoder(src_tensor, src_mask)\n","\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n","\n","    for i in range(max_len):\n","\n","        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n","\n","        trg_mask = model.make_trg_mask(trg_tensor)\n","        \n","        with torch.no_grad():\n","            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n","        \n","        pred_token = output.argmax(2)[:,-1].item()\n","        \n","        trg_indexes.append(pred_token)\n","\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","            break\n","    \n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","    \n","    return trg_tokens[1:], attention"]},{"cell_type":"markdown","metadata":{"id":"Z1cS9HumILkm"},"source":["Теперь определим функцию, которая отображает внимание над исходным предложением для каждого шага декодирования. Поскольку эта модель имеет 8 голов, в модели можно просматривать внимание для каждой из голов."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CctZTvYILkm","executionInfo":{"status":"aborted","timestamp":1654037176722,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n","    \n","    assert n_rows * n_cols == n_heads\n","    \n","    fig = plt.figure(figsize=(15,25))\n","    \n","    for i in range(n_heads):\n","        \n","        ax = fig.add_subplot(n_rows, n_cols, i+1)\n","        \n","        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n","\n","        cax = ax.matshow(_attention, cmap='bone')\n","\n","        ax.tick_params(labelsize=12)\n","        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n","                           rotation=45)\n","        ax.set_yticklabels(['']+translation)\n","\n","        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","    plt.show()\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"Vf6VEK9mILkn"},"source":["Во-первых, возьмем пример из обучающего набора."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDCYniP0ILkn","executionInfo":{"status":"aborted","timestamp":1654037176723,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["example_idx = 8\n","\n","src = vars(train_data.examples[example_idx])['src']\n","trg = vars(train_data.examples[example_idx])['trg']\n","\n","print(f'src = {src}')\n","print(f'trg = {trg}')"]},{"cell_type":"markdown","metadata":{"id":"EjBl-aEgILko"},"source":["Наш перевод выглядит довольно хорошо, хотя наша модель меняет *walking by * на *walks past*. Смысл все тот же."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuHoKzMsILko","executionInfo":{"status":"aborted","timestamp":1654037176723,"user_tz":-480,"elapsed":73,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["translation, attention = translate_sentence(src, SRC, TRG, model, device)\n","\n","print(f'predicted trg = {translation}')"]},{"cell_type":"markdown","metadata":{"id":"RToqVMJEILkp"},"source":["Мы можем видеть внимание от каждой головы ниже. Каждая из них, безусловно, отличается, но трудно (возможно, невозможно) рассуждать о том, на что head на самом деле научился обращать внимание."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvbdWqQqILkp","executionInfo":{"status":"aborted","timestamp":1654037176725,"user_tz":-480,"elapsed":75,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["display_attention(src, translation, attention)"]},{"cell_type":"markdown","metadata":{"id":"7slPUewdILkq"},"source":["Далее возьмем пример, на котором модель не была обучена, из валидационного набора."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-raYSbwKILkq","executionInfo":{"status":"aborted","timestamp":1654037176725,"user_tz":-480,"elapsed":74,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["example_idx = 6\n","\n","src = vars(valid_data.examples[example_idx])['src']\n","trg = vars(valid_data.examples[example_idx])['trg']\n","\n","print(f'src = {src}')\n","print(f'trg = {trg}')"]},{"cell_type":"markdown","metadata":{"id":"AfIldpxmILkr"},"source":["Модель переводит, заменяя *after * на *behind*, что по смыслу не совсем верно. Но русский язык многозначен, поэтому может быть воспринято и так."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBeCVqggILks","executionInfo":{"status":"aborted","timestamp":1654037176726,"user_tz":-480,"elapsed":75,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["translation, attention = translate_sentence(src, SRC, TRG, model, device)\n","\n","print(f'predicted trg = {translation}')"]},{"cell_type":"markdown","metadata":{"id":"fHQgTG8OILkt"},"source":["Ситуация аналогична"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DIOYrJtILkt","executionInfo":{"status":"aborted","timestamp":1654037176726,"user_tz":-480,"elapsed":75,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["display_attention(src, translation, attention)"]},{"cell_type":"markdown","metadata":{"id":"lZOYqQ49ILkt"},"source":["Наконец, рассмотрим пример из тестовой выборки"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urBW-1pCILku","executionInfo":{"status":"aborted","timestamp":1654037176727,"user_tz":-480,"elapsed":75,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["example_idx = 10\n","\n","src = vars(test_data.examples[example_idx])['src']\n","trg = vars(test_data.examples[example_idx])['trg']\n","\n","print(f'src = {src}')\n","print(f'trg = {trg}')"]},{"cell_type":"markdown","metadata":{"id":"E6pnA31SILku"},"source":["Тут скорее всего была ошибка в разметке. Вместо song должно было son. И модель устойчива к данной опечатке!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubTgY0utILkv","executionInfo":{"status":"aborted","timestamp":1654037176727,"user_tz":-480,"elapsed":75,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["translation, attention = translate_sentence(src, SRC, TRG, model, device)\n","\n","print(f'predicted trg = {translation}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4uCnU8JILkv","executionInfo":{"status":"aborted","timestamp":1654037176727,"user_tz":-480,"elapsed":75,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["display_attention(src, translation, attention)"]},{"cell_type":"markdown","metadata":{"id":"RTftiIlBILkw"},"source":["## BLEU\n","\n","Рассчитываем оценку BLEU для трансформера (чем больше, тем лучше)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSo_825AILkw","executionInfo":{"status":"aborted","timestamp":1654037176728,"user_tz":-480,"elapsed":75,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["from torchtext.data.metrics import bleu_score\n","from tqdm.auto import tqdm\n","\n","def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n","    \n","    trgs = []\n","    pred_trgs = []\n","    pbar = tqdm(total=len(data))\n","    for datum in data:\n","        \n","        src = vars(datum)['src']\n","        trg = vars(datum)['trg']\n","        \n","        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n","        \n","        #cut off <eos> token\n","        pred_trg = pred_trg[:-1]\n","        \n","        pred_trgs.append(pred_trg)\n","        trgs.append([trg])\n","        pbar.update(1)\n","        \n","    return bleu_score(pred_trgs, trgs)"]},{"cell_type":"markdown","metadata":{"id":"OhSPVUQSILkx"},"source":["Мы получаем оценку BLEU в 42 балла, что превышает ~ 40 баллов сверточной модели. Все это при меньшем количестве параметров и более быстром времени обучения!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rxZsAU49ILkx","executionInfo":{"status":"aborted","timestamp":1654037176728,"user_tz":-480,"elapsed":75,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n","\n","print(f'BLEU score = {bleu_score*100:.2f}')"]},{"cell_type":"markdown","metadata":{"id":"WXBAhLeIILky"},"source":["## Приложение\n","\n","Выше приведённая функция `calculate_bleu` не оптимизирована. Ниже представлена намного более быстрая, векторизованная версия. Автор реализации [@azadyasar](https://github.com/azadyasar)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dO9BHA4VILky","executionInfo":{"status":"aborted","timestamp":1654037176729,"user_tz":-480,"elapsed":76,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["def translate_sentence_vectorized(src_tensor, src_field, trg_field, model, device, max_len=50):\n","    assert isinstance(src_tensor, torch.Tensor)\n","\n","    model.eval()\n","    src_mask = model.make_src_mask(src_tensor)\n","\n","    with torch.no_grad():\n","        enc_src = model.encoder(src_tensor, src_mask)\n","    # enc_src = [batch_sz, src_len, hid_dim]\n","\n","    trg_indexes = [[trg_field.vocab.stoi[trg_field.init_token]] for _ in range(len(src_tensor))]\n","    # Even though some examples might have been completed by producing a <eos> token\n","    # we still need to feed them through the model because other are not yet finished\n","    # and all examples act as a batch. Once every single sentence prediction encounters\n","    # <eos> token, then we can stop predicting.\n","    translations_done = [0] * len(src_tensor)\n","    for i in range(max_len):\n","        trg_tensor = torch.LongTensor(trg_indexes).to(device)\n","        trg_mask = model.make_trg_mask(trg_tensor)\n","        with torch.no_grad():\n","            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n","        pred_tokens = output.argmax(2)[:,-1]\n","        for i, pred_token_i in enumerate(pred_tokens):\n","            trg_indexes[i].append(pred_token_i)\n","            if pred_token_i == trg_field.vocab.stoi[trg_field.eos_token]:\n","                translations_done[i] = 1\n","        if all(translations_done):\n","            break\n","\n","    # Iterate through each predicted example one by one;\n","    # Cut-off the portion including the after the <eos> token\n","    pred_sentences = []\n","    for trg_sentence in trg_indexes:\n","        pred_sentence = []\n","        for i in range(1, len(trg_sentence)):\n","            if trg_sentence[i] == trg_field.vocab.stoi[trg_field.eos_token]:\n","                break\n","            pred_sentence.append(trg_field.vocab.itos[trg_sentence[i]])\n","        pred_sentences.append(pred_sentence)\n","\n","    return pred_sentences, attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DX3sPs5KILkz","executionInfo":{"status":"aborted","timestamp":1654037176729,"user_tz":-480,"elapsed":75,"user":{"displayName":"王超超","userId":"00933822056115994187"}}},"outputs":[],"source":["from torchtext.data.metrics import bleu_score\n","\n","def calculate_bleu_alt(iterator, src_field, trg_field, model, device, max_len = 50):\n","    trgs = []\n","    pred_trgs = []\n","    with torch.no_grad():\n","        for batch in iterator:\n","            src = batch.src\n","            trg = batch.trg\n","            _trgs = []\n","            for sentence in trg:\n","                tmp = []\n","                # Start from the first token which skips the <start> token\n","                for i in sentence[1:]:\n","                    # Targets are padded. So stop appending as soon as a padding or eos token is encountered\n","                    if i == trg_field.vocab.stoi[trg_field.eos_token] or i == trg_field.vocab.stoi[trg_field.pad_token]:\n","                        break\n","                    tmp.append(trg_field.vocab.itos[i])\n","                _trgs.append([tmp])\n","            trgs += _trgs\n","            pred_trg, _ = translate_sentence_vectorized(src, src_field, trg_field, model, device)\n","            pred_trgs += pred_trg\n","    return pred_trgs, trgs, bleu_score(pred_trgs, trgs)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"“Lab8.ipynb”的副本","provenance":[{"file_id":"https://github.com/iu5git/Deep-learning/blob/main/notebooks/Lab8.ipynb","timestamp":1653898016500}],"collapsed_sections":["WXBAhLeIILky"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}